# Person 3: Embeddings & Vector Search âœ… COMPLETE

Generates embeddings for document chunks and provides vector search functionality via Flask REST API.

## ğŸ“‹ Status

**âœ… COMPLETE & TESTED**

- Embedding generation working
- FAISS indexing implemented
- Flask API running on port 5001
- Unit tests: 5/5 passing
- Ready for Person 4 integration

## ğŸ¯ Responsibilities

1. Load chunks from Person 2's output (`../storage/chunks/`)
2. Generate embeddings using `all-MiniLM-L6-v2`
3. Build FAISS index (FlatIP for cosine similarity)
4. Provide REST API for vector search
5. Support metadata filtering

## ğŸ—ï¸ Files

```
embed-and-vec-search/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ embed_and_index.py       # Main indexing script
â”œâ”€â”€ vector_search_api.py     # Flask REST API
â”œâ”€â”€ test_embeddings.py       # Unit tests
â”œâ”€â”€ vector_index/            # Generated index files
â”‚   â”œâ”€â”€ faiss.index
â”‚   â””â”€â”€ metadata.pkl
â””â”€â”€ README.md
```

## ğŸš€ Usage

### 1. Generate Embeddings & Build Index

```bash
python embed_and_index.py
```

This will:
- Load all chunks from `../storage/chunks/`
- Generate 384-dimensional embeddings
- Build FAISS index
- Save to `vector_index/`

Expected output:
```
==============================================================
PERSON 3: Embeddings & Vector Index
==============================================================

[1/4] Loading chunks from storage/chunks/...
âœ… Loaded 245 chunks

[2/4] Generating embeddings...
âœ… Generated embeddings with shape: (245, 384)

[3/4] Building FAISS index...
âœ… Built index with 245 vectors

[4/4] Saving index to disk...
âœ… Index saved successfully
```

### 2. Start Vector Search API

```bash
python vector_search_api.py
```

API will run on `http://localhost:5001`

### 3. Test the API

**Health Check**:
```bash
curl http://localhost:5001/health
```

**Search for Chunks**:
```bash
curl -X POST http://localhost:5001/vector/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "GMP compliance requirements",
    "k": 5
  }'
```

**Rebuild Index**:
```bash
curl -X POST http://localhost:5001/vector/index
```

## ğŸ“¡ API Reference

### POST /vector/search

Search for similar chunks.

**Request**:
```json
{
  "query": "What are GMP requirements?",
  "k": 5,
  "filters": {
    "doc_id": "optional_doc_id_filter"
  }
}
```

**Response**:
```json
{
  "query": "What are GMP requirements?",
  "results": [
    {
      "chunk_id": "7ba118f3_chunk_0",
      "doc_id": "7ba118f3...",
      "chunk_index": 0,
      "text": "Full chunk text...",
      "score": 0.8544,
      "source": "document.pdf",
      "page": 1,
      "start_offset": 0,
      "end_offset": 500
    }
  ],
  "count": 5
}
```

### POST /vector/index

Rebuild the vector index from scratch (use after new documents are ingested).

**Response**:
```json
{
  "status": "success",
  "num_chunks": 245,
  "index_size": 245
}
```

### GET /health

Health check endpoint.

**Response**:
```json
{
  "status": "healthy",
  "index_size": 245,
  "index_loaded": true
}
```

## ğŸ”Œ Integration with Person 4 (RAG Orchestrator)

Person 4 will call this API to retrieve relevant chunks:

```python
import requests

def retrieve_relevant_chunks(query: str, k: int = 5):
    """Retrieve relevant chunks for RAG."""
    response = requests.post(
        'http://localhost:5001/vector/search',
        json={'query': query, 'k': k},
        timeout=10
    )
    return response.json()['results']

# Usage
chunks = retrieve_relevant_chunks("What are GMP requirements?")
for chunk in chunks:
    print(f"Score: {chunk['score']:.4f}")
    print(f"Text: {chunk['text'][:100]}...")
```

## ğŸ§ª Testing

Run unit tests:
```bash
pytest test_embeddings.py -v
```

Expected output:
```
test_embeddings.py::test_load_chunks PASSED
test_embeddings.py::test_embed_chunks PASSED
test_embeddings.py::test_build_and_search PASSED
test_embeddings.py::test_search_with_filters PASSED
test_embeddings.py::test_save_and_load_index PASSED

========== 5 passed in 12.34s ==========
```

## ğŸ”§ Technical Details

### Embedding Model
- **Model**: `all-MiniLM-L6-v2` (sentence-transformers)
- **Dimension**: 384
- **Speed**: ~100 chunks/second
- **Size**: ~90MB

### FAISS Index
- **Type**: FlatIP (exact cosine similarity)
- **Normalization**: L2-normalized embeddings
- **Storage**: Persistent on disk
- **Query Speed**: <100ms for 10K vectors

### Performance
- **Indexing**: ~1 second per 100 chunks
- **Search**: <1 second per query
- **Memory**: ~50MB for 10K chunks
- **Disk**: ~100MB for index + metadata

## ğŸ“¦ Dependencies

```txt
sentence-transformers==2.2.2  # Embedding model
faiss-cpu==1.7.4             # Vector search
flask==3.0.0                 # REST API
numpy==1.24.3                # Array operations
python-dotenv                # Environment variables
```

## ğŸ› Troubleshooting

**"No chunks found"**
- Ensure Person 2 has run ingestion first
- Check `../storage/chunks/` directory exists and has content

**"ModuleNotFoundError: sentence_transformers"**
```bash
pip install sentence-transformers==2.2.2
```

**"FAISS index returns no results"**
- Rebuild index: `curl -X POST http://localhost:5001/vector/index`
- Check that embeddings were generated successfully

**"API not responding"**
```bash
# Check if process is running
lsof -i :5001  # Mac/Linux
netstat -ano | findstr :5001  # Windows

# Restart API
python vector_search_api.py
```

## ğŸ“Š Example Usage

### Complete workflow:

```bash
# 1. Person 2 ingests document
cd ../ingestion
python ingest.py "document.pdf"

# 2. Build embeddings & index
cd ../embed-and-vec-search
python embed_and_index.py

# 3. Start API
python vector_search_api.py

# 4. Test search (in another terminal)
curl -X POST http://localhost:5001/vector/search \
  -H "Content-Type: application/json" \
  -d '{"query": "FDA requirements", "k": 3}'
```

## ğŸ¤ Handoff to Person 4

**What Person 4 needs to know**:

1. **API Endpoint**: `http://localhost:5001/vector/search`
2. **Request Format**: `{"query": str, "k": int, "filters": dict}`
3. **Response**: Array of chunks with text, score, and metadata
4. **Integration**: Use Python `requests` library or any HTTP client

**Example integration**:
```python
# In Person 4's orchestrator.py
from typing import List, Dict
import requests

def build_rag_context(user_query: str, max_chunks: int = 5) -> str:
    """Build context from relevant chunks for RAG."""
    response = requests.post(
        'http://localhost:5001/vector/search',
        json={'query': user_query, 'k': max_chunks}
    )
    
    chunks = response.json()['results']
    
    context_parts = []
    for i, chunk in enumerate(chunks, 1):
        context_parts.append(
            f"[Source {i}: {chunk['source']}, "
            f"Page {chunk['page']}, Score: {chunk['score']:.2f}]\n"
            f"{chunk['text']}\n"
        )
    
    return "\n---\n".join(context_parts)

# Usage in RAG pipeline
query = "What are GMP requirements?"
context = build_rag_context(query)
# Now pass context + query to LLM (Person 5)
```

## âœ… Acceptance Criteria

All criteria met:

- [x] Loads chunks from Person 2's output
- [x] Generates embeddings (shape: num_chunks Ã— 384)
- [x] Builds FAISS index
- [x] Provides REST API on port 5001
- [x] Search returns results in <2 seconds
- [x] Supports metadata filtering
- [x] All 5 unit tests pass
- [x] API documentation complete
- [x] Ready for Person 4 integration

---

**Status**: âœ… Complete and Production-Ready  
**Maintained by**: Person 3  
**Last Updated**: December 2025  
**API**: http://localhost:5001

